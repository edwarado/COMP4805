{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e867a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from scipy) (1.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55776df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: filelock in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb73d494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/apple/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f078dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9247106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a330f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7569cb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9035c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4ce0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "301a9a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5382335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(score, alpha=15):\n",
    "    \"\"\"\n",
    "    Normalize the score to be between -1 and 1 using an alpha that\n",
    "    approximates the max expected value\n",
    "    \"\"\"\n",
    "    norm_score = score/math.sqrt((score*score) + alpha)\n",
    "    return norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fb709f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_scores_roberta(example):\n",
    "    encoded_text = tokenizer(example, return_tensors='pt')\n",
    "    output = model(**encoded_text)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    print(scores)\n",
    "    total = scores[0]*-1 + scores[1] + scores[2]\n",
    "    print(total)\n",
    "    print(normalize(total))\n",
    "    return normalize(total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edd2984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news = pd.read_csv('bitcoin_news_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae4c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb635512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a43f933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_news[\"sentiment2\"] = bitcoin_news[\"sentiment\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae3a2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02668336 0.796488   0.17682865]\n",
      "0.9466332755982876\n",
      "0.23743037286860103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23743037286860103"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_scores_roberta(bitcoin_news[\"healine\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfb6fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_neg=[]\n",
    "vader_neu=[]\n",
    "vader_pos=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be227d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02668336 0.796488   0.17682865]\n",
      "0.9466332755982876\n",
      "0.23743037286860103\n",
      "[0.00322191 0.6174052  0.37937284]\n",
      "0.993556106230244\n",
      "0.24848880136013016\n",
      "[0.01353379 0.86036295 0.12610321]\n",
      "0.9729323610663414\n",
      "0.24364002434734505\n",
      "[0.00724834 0.8968408  0.095911  ]\n",
      "0.9855034681968391\n",
      "0.24659777322209608\n",
      "[0.04159305 0.9209553  0.0374517 ]\n",
      "0.9168139472603798\n",
      "0.23035418836402896\n",
      "[0.05846324 0.8486479  0.09288884]\n",
      "0.8830734938383102\n",
      "0.2223032660691781\n",
      "[0.00738567 0.9216081  0.0710062 ]\n",
      "0.9852286214008927\n",
      "0.24653318014625605\n",
      "[0.49896598 0.48468807 0.01634595]\n",
      "0.002068042755126953\n",
      "0.000533966267201199\n",
      "[0.03303174 0.9233671  0.04360105]\n",
      "0.9339364022016525\n",
      "0.23442192650777716\n",
      "[0.00586437 0.65058136 0.3435541 ]\n",
      "0.9882711018435657\n",
      "0.24724802493344297\n",
      "[0.02373973 0.9252956  0.05096471]\n",
      "0.9525205697864294\n",
      "0.23882302205197317\n",
      "[0.2507838  0.7270784  0.02213774]\n",
      "0.4984323177486658\n",
      "0.12764198825139886\n",
      "[0.10435191 0.8070086  0.08863941]\n",
      "0.791296124458313\n",
      "0.20017648103749366\n",
      "[0.03874179 0.8696053  0.09165281]\n",
      "0.9225163199007511\n",
      "0.23171023984089004\n",
      "[0.03708164 0.8572992  0.1056191 ]\n",
      "0.9258366748690605\n",
      "0.2324992148853656\n",
      "[0.08570783 0.8722895  0.04200269]\n",
      "0.8285843394696712\n",
      "0.20920544024337456\n",
      "[0.01496576 0.71972036 0.26531383]\n",
      "0.9700684351846576\n",
      "0.24296524042767115\n",
      "[0.5632801  0.41811267 0.01860734]\n",
      "-0.12656010314822197\n",
      "-0.03266024496656787\n",
      "[0.01164881 0.5150368  0.47331434]\n",
      "0.9767023520544171\n",
      "0.24452775154227824\n",
      "[0.84630615 0.1464276  0.00726623]\n",
      "-0.6926123136654496\n",
      "-0.1760389513341382\n",
      "[0.00204089 0.01144477 0.98651433]\n",
      "0.9959182105958462\n",
      "0.2490429648647195\n",
      "[0.00464798 0.7247035  0.27064857]\n",
      "0.9907040842808783\n",
      "0.24781937506300564\n",
      "[0.00435596 0.60542727 0.39021674]\n",
      "0.9912880444899201\n",
      "0.24795647108090954\n",
      "[0.00425627 0.437504   0.55823976]\n",
      "0.9914874853566289\n",
      "0.24800329029168547\n",
      "[0.06507256 0.8940474  0.04088009]\n",
      "0.8698549121618271\n",
      "0.21913659527226545\n",
      "[0.00380969 0.28562352 0.7105667 ]\n",
      "0.9923805340658873\n",
      "0.24821291425381198\n",
      "[0.08270452 0.82578826 0.09150717]\n",
      "0.8345909118652344\n",
      "0.21065494488087577\n",
      "[0.00499463 0.8004556  0.19454971]\n",
      "0.9900106489658356\n",
      "0.24765655829689653\n",
      "[0.21401694 0.6657853  0.12019772]\n",
      "0.5719660893082619\n",
      "0.14609644396574956\n",
      "[0.01115675 0.37774163 0.61110157]\n",
      "0.9776864564046264\n",
      "0.24475937946641607\n",
      "[0.30373305 0.66385186 0.03241511]\n",
      "0.39253391698002815\n",
      "0.10083524526926245\n",
      "[0.01773676 0.9052397  0.07702347]\n",
      "0.9645264092832804\n",
      "0.24165845441389122\n",
      "[0.00354442 0.6225853  0.37387022]\n",
      "0.9929111020173877\n",
      "0.24833743715246065\n",
      "[0.00497157 0.2603874  0.7346411 ]\n",
      "0.990056895185262\n",
      "0.24766741743932721\n",
      "[0.01741697 0.92784315 0.0547399 ]\n",
      "0.9651660919189453\n",
      "0.24180935613596571\n",
      "[0.02021202 0.8978319  0.08195595]\n",
      "0.9595758505165577\n",
      "0.24049002072038209\n",
      "[0.05689938 0.9117815  0.03131919]\n",
      "0.8862013034522533\n",
      "0.22305154653431938\n",
      "[0.01929114 0.91685855 0.06385033]\n",
      "0.9614177383482456\n",
      "0.24092486657087\n",
      "[0.00363694 0.5405593  0.4558038 ]\n",
      "0.9927261660341173\n",
      "0.248294034562406\n",
      "[0.52637273 0.45164838 0.02197892]\n",
      "-0.05274542234838009\n",
      "-0.013617546711525999\n",
      "[0.4151714  0.55290526 0.03192336]\n",
      "0.16965720802545547\n",
      "0.043763334043224474\n",
      "[0.01807047 0.8897652  0.09216436]\n",
      "0.9638590961694717\n",
      "0.24150101594972712\n",
      "[0.00289184 0.39157885 0.6055294 ]\n",
      "0.9942164439707994\n",
      "0.24864374505661835\n",
      "[0.53105164 0.444276   0.02467237]\n",
      "-0.0621032640337944\n",
      "-0.01603293275361109\n",
      "[0.01896808 0.9224505  0.05858161]\n",
      "0.9620640203356743\n",
      "0.24107741100879537\n",
      "[0.0111299  0.84388113 0.1449891 ]\n",
      "0.9777403306216002\n",
      "0.24477205859227133\n",
      "[0.03039399 0.8525987  0.11700719]\n",
      "0.9392119217664003\n",
      "0.2356727520417586\n",
      "[0.15321419 0.8255723  0.0212135 ]\n",
      "0.6935716271400452\n",
      "0.1762752055262167\n",
      "[0.00265973 0.11712097 0.8802193 ]\n",
      "0.9946805217768997\n",
      "0.2487526262936591\n",
      "[0.01423889 0.7321365  0.25362453]\n",
      "0.9715221310034394\n",
      "0.24330779712768993\n",
      "[0.00224191 0.28747633 0.7102817 ]\n",
      "0.9955161528196186\n",
      "0.24894865701387922\n",
      "[0.619772   0.34334958 0.03687843]\n",
      "-0.23954401165246964\n",
      "-0.0617320350113256\n",
      "[0.00814237 0.80614674 0.18571098]\n",
      "0.9837153498083353\n",
      "0.24617748003484144\n",
      "[0.04736513 0.9158996  0.03673537]\n",
      "0.9052698239684105\n",
      "0.22760483465377526\n",
      "[0.00461566 0.7952701  0.20011435]\n",
      "0.9907687809318304\n",
      "0.24783456459378642\n",
      "[0.02499284 0.8990881  0.07591909]\n",
      "0.9500143341720104\n",
      "0.23823034743653004\n",
      "[0.02768501 0.85178417 0.12053068]\n",
      "0.9446298349648714\n",
      "0.23695612112484185\n",
      "[0.27279368 0.6922031  0.03500324]\n",
      "0.45441266894340515\n",
      "0.11652951210134245\n",
      "[0.13255695 0.80925924 0.05818395]\n",
      "0.7348862364888191\n",
      "0.18642055604591615\n",
      "[0.00700485 0.8259757  0.16701949]\n",
      "0.9859903557226062\n",
      "0.24671219094848418\n",
      "[0.0357887  0.85745823 0.10675307]\n",
      "0.9284226037561893\n",
      "0.23311335962140223\n",
      "[0.14117283 0.8406225  0.01820463]\n",
      "0.7176542915403843\n",
      "0.18219607164733184\n",
      "[0.12319915 0.78318167 0.0936192 ]\n",
      "0.7536017149686813\n",
      "0.19099704346919996\n",
      "[0.00294065 0.26230615 0.73475313]\n",
      "0.9941186378709972\n",
      "0.2486207967337456\n",
      "[0.9256692  0.07059588 0.00373481]\n",
      "-0.8513385080732405\n",
      "-0.21468911556004297\n",
      "[0.00368831 0.5105668  0.485745  ]\n",
      "0.9926234784070402\n",
      "0.2482699341773943\n",
      "[0.74490374 0.24524646 0.00984967]\n",
      "-0.48980761505663395\n",
      "-0.125468385705015\n",
      "[0.05558141 0.86842406 0.07599455]\n",
      "0.8888371959328651\n",
      "0.2236818374239105\n",
      "[0.65958405 0.3150506  0.02536545]\n",
      "-0.3191679921001196\n",
      "-0.08213041050082519\n",
      "[0.01702652 0.8462186  0.13675483]\n",
      "0.9659468941390514\n",
      "0.24199352432214824\n",
      "[0.00344424 0.5024377  0.49411803]\n",
      "0.9931115086656064\n",
      "0.24838446886132676\n",
      "[0.30316788 0.657598   0.03923411]\n",
      "0.39366424828767776\n",
      "0.10112264306807664\n",
      "[0.33021343 0.6391722  0.03061437]\n",
      "0.33957314118742943\n",
      "0.08734233617351558\n",
      "[0.00357618 0.22497348 0.7714502 ]\n",
      "0.9928475297056139\n",
      "0.24832251754762494\n",
      "[0.0212328  0.75937915 0.21938805]\n",
      "0.9575343988835812\n",
      "0.2400078912137872\n",
      "[0.15180579 0.82328385 0.02491036]\n",
      "0.6963884215801954\n",
      "0.17696873326503576\n",
      "[0.00883376 0.88035107 0.1108152 ]\n",
      "0.9823325118049979\n",
      "0.24585235181793888\n",
      "[0.40595472 0.56250054 0.0315447 ]\n",
      "0.1880905218422413\n",
      "0.04850759414037094\n",
      "[0.2813971  0.68791336 0.03068954]\n",
      "0.4372057970613241\n",
      "0.1121735853764932\n",
      "[0.00435766 0.4820199  0.51362234]\n",
      "0.9912845869548619\n",
      "0.24795565940113082\n",
      "[0.4045407  0.58121926 0.01424001]\n",
      "0.1909185741096735\n",
      "0.04923517958178902\n",
      "[0.03136324 0.8884929  0.08014382]\n",
      "0.9372734688222408\n",
      "0.235213280640014\n",
      "[0.00976329 0.47945124 0.51078546]\n",
      "0.980473411269486\n",
      "0.24541511556619963\n",
      "[0.01828769 0.62886393 0.35284826]\n",
      "0.9634245056658983\n",
      "0.2413984732244741\n",
      "[0.01482342 0.95080185 0.03437471]\n",
      "0.9703531349077821\n",
      "0.24303233576410446\n",
      "[0.00572101 0.47277412 0.5215049 ]\n",
      "0.9885579845868051\n",
      "0.24731540855167483\n",
      "[0.04609878 0.92317605 0.03072521]\n",
      "0.907802477478981\n",
      "0.22820848140113179\n",
      "[0.00370925 0.57127064 0.42502022]\n",
      "0.9925816166214645\n",
      "0.24826010924753436\n",
      "[0.0186493  0.8254034  0.15594728]\n",
      "0.9627013709396124\n",
      "0.24122782984056731\n",
      "[0.04566293 0.9181693  0.0361677 ]\n",
      "0.9086740873754025\n",
      "0.22841616494263872\n",
      "[0.01322974 0.85262537 0.13414484]\n",
      "0.9735404765233397\n",
      "0.24378325998419734\n",
      "[0.09792066 0.7414227  0.16065653]\n",
      "0.8041585758328438\n",
      "0.20329688402150795\n",
      "[0.0175725  0.90761125 0.07481615]\n",
      "0.9648549016565084\n",
      "0.24173594825758915\n",
      "[0.02344337 0.8655846  0.11097211]\n",
      "0.9531133566051722\n",
      "0.23896316529046177\n",
      "[0.06824683 0.85998034 0.0717729 ]\n",
      "0.8635064139962196\n",
      "0.21761326400730147\n",
      "[0.02098366 0.9098551  0.06916124]\n",
      "0.958032701164484\n",
      "0.24012559163367606\n",
      "[0.10478988 0.8503965  0.04481364]\n",
      "0.7904202677309513\n",
      "0.19996377723786632\n",
      "[0.5235062  0.4510674  0.02542648]\n",
      "-0.04701235145330429\n",
      "-0.012137642777683188\n",
      "[0.06471181 0.6980127  0.23727547]\n",
      "0.8705763667821884\n",
      "0.21930960821552076\n",
      "[0.00527547 0.62566084 0.36906374]\n",
      "0.9894491024315357\n",
      "0.24752469328711388\n",
      "[0.01688096 0.93803036 0.04508857]\n",
      "0.9662379734218121\n",
      "0.24206217463816035\n",
      "[0.02340401 0.9112475  0.0653486 ]\n",
      "0.9531920831650496\n",
      "0.2389817762463883\n",
      "[0.02052711 0.93607706 0.04339563]\n",
      "0.9589455779641867\n",
      "0.24034118826512949\n",
      "[0.56547046 0.42289343 0.01163614]\n",
      "-0.13094088342040777\n",
      "-0.033789484966215755\n",
      "[0.00453879 0.61433244 0.3811288 ]\n",
      "0.9909224309958518\n",
      "0.24787063794463363\n",
      "[0.07316341 0.5827107  0.3441259 ]\n",
      "0.8536731749773026\n",
      "0.21525062495274006\n",
      "[0.6607069  0.31381124 0.02548194]\n",
      "-0.32141369953751564\n",
      "-0.08270435174246825\n",
      "[0.8573752  0.1368378  0.00578693]\n",
      "-0.714750477578491\n",
      "-0.18148318807255687\n",
      "[0.5539808  0.4274221  0.01859709]\n",
      "-0.10796163231134415\n",
      "-0.02786474958192464\n",
      "[0.6809473  0.31181055 0.00724231]\n",
      "-0.3618944394402206\n",
      "-0.09303547080547389\n",
      "[0.7582045  0.23266229 0.00913318]\n",
      "-0.5164090450853109\n",
      "-0.1321665529741304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02099832 0.4359234  0.5430783 ]\n",
      "0.9580033775418997\n",
      "0.24011866560359404\n",
      "[0.00949753 0.6358407  0.35466176]\n",
      "0.9810049505904317\n",
      "0.245540142000266\n",
      "[0.5351696  0.44450644 0.02032403]\n",
      "-0.07033913768827915\n",
      "-0.018158492808609667\n",
      "[0.08881183 0.8772381  0.03394999]\n",
      "0.8223762586712837\n",
      "0.20770585717018836\n",
      "[0.0059397  0.87932837 0.11473174]\n",
      "0.9881204147823155\n",
      "0.24721262980453024\n",
      "[0.01264001 0.62850773 0.35885227]\n",
      "0.9747199928388\n",
      "0.24406103801324522\n",
      "[0.00627725 0.887833   0.10588977]\n",
      "0.9874455230310559\n",
      "0.24705409124287805\n",
      "[0.00465657 0.9033211  0.09202234]\n",
      "0.9906868580728769\n",
      "0.2478153306494837\n",
      "[0.01126434 0.7914222  0.1973134 ]\n",
      "0.9774712435901165\n",
      "0.24470872856142073\n",
      "[0.00661498 0.8129333  0.1804515 ]\n",
      "0.9867698424495757\n",
      "0.24689534750048883\n",
      "[0.00330216 0.314897   0.6818009 ]\n",
      "0.9933957431931049\n",
      "0.24845117040018813\n",
      "[0.04856751 0.824422   0.12701046]\n",
      "0.9028649590909481\n",
      "0.22703140276907544\n",
      "[0.01831815 0.92115796 0.0605238 ]\n",
      "0.9633636064827442\n",
      "0.24138410326257942\n",
      "[0.00562432 0.7275742  0.26680145]\n",
      "0.9887513522990048\n",
      "0.24736082513971064\n",
      "[0.05332341 0.8644149  0.0822617 ]\n",
      "0.8933531641960144\n",
      "0.22476103708272616\n",
      "[0.71513295 0.27193603 0.01293094]\n",
      "-0.4302659798413515\n",
      "-0.1104149216442105\n",
      "[0.32101342 0.658523   0.02046351]\n",
      "0.357973113656044\n",
      "0.09203596570464725\n",
      "[0.54084396 0.41914436 0.04001171]\n",
      "-0.08168788999319077\n",
      "-0.021087032626897175\n",
      "[0.58484143 0.399899   0.01525957]\n",
      "-0.1696828557178378\n",
      "-0.043769937230601644\n",
      "[0.08025282 0.8798013  0.03994585]\n",
      "0.8394943028688431\n",
      "0.2118372012794559\n",
      "[0.02073045 0.9235402  0.05572926]\n",
      "0.9585389848798513\n",
      "0.24024516644600344\n",
      "[0.0100412  0.92182565 0.06813318]\n",
      "0.9799176333472133\n",
      "0.24528437473282838\n",
      "[0.35858586 0.6189205  0.02249355]\n",
      "0.28282819129526615\n",
      "0.07283198451510711\n",
      "[0.02312994 0.8057829  0.17108709]\n",
      "0.9537400603294373\n",
      "0.23911131073109262\n",
      "[0.7243489  0.2589236  0.01672755]\n",
      "-0.44869776256382465\n",
      "-0.11508351416456132\n",
      "[0.41671595 0.565023   0.01826119]\n",
      "0.16656824201345444\n",
      "0.042968015290553864\n",
      "[0.00626079 0.7099863  0.28375295]\n",
      "0.9874784885905683\n",
      "0.24706183563078835\n",
      "[0.05169702 0.79037005 0.15793301]\n",
      "0.8966060355305672\n",
      "0.22553787735612718\n",
      "[0.2750014  0.6183705  0.10662805]\n",
      "0.44999711960554123\n",
      "0.11541234488836354\n",
      "[0.06544311 0.85993344 0.0746234 ]\n",
      "0.8691137209534645\n",
      "0.21895882774328945\n",
      "[0.0411742  0.9207218  0.03810393]\n",
      "0.9176515564322472\n",
      "0.23055346014342312\n",
      "[0.39528006 0.56229687 0.04242294]\n",
      "0.2094397433102131\n",
      "0.05399821243241568\n",
      "[0.01038244 0.79274523 0.19687223]\n",
      "0.9792350269854069\n",
      "0.2451237805465187\n",
      "[0.06962475 0.878295   0.05208025]\n",
      "0.8607505038380623\n",
      "0.21695148335496975\n",
      "[0.00963012 0.8756835  0.11468641]\n",
      "0.9807397723197937\n",
      "0.24547776941866895\n",
      "[0.5868508  0.3938405  0.01930865]\n",
      "-0.1737016774713993\n",
      "-0.04480454108006255\n",
      "[0.00879095 0.81613666 0.17507242]\n",
      "0.9824181264266372\n",
      "0.24587248365081543\n",
      "[0.30285767 0.6552301  0.04191216]\n",
      "0.39428460225462914\n",
      "0.10128036338178528\n",
      "[0.40765753 0.5707208  0.02162169]\n",
      "0.18468494713306427\n",
      "0.04763132457169126\n",
      "[0.01177829 0.8800855  0.10813596]\n",
      "0.9764431994408369\n",
      "0.24446674801465113\n",
      "[0.05784594 0.9112086  0.03094555]\n",
      "0.8843082450330257\n",
      "0.22259870836134263\n",
      "[0.7623566  0.23079549 0.006848  ]\n",
      "-0.524713090620935\n",
      "-0.1342538306678109\n",
      "[0.16516984 0.7935908  0.04123939]\n",
      "0.6696603409945965\n",
      "0.17037748160938818\n",
      "[0.48710892 0.44018832 0.07270292]\n",
      "0.025782324373722076\n",
      "0.006656820030819277\n",
      "[0.3069011  0.65282136 0.04027756]\n",
      "0.3861978277564049\n",
      "0.0992237671488689\n",
      "[0.5000334  0.48586857 0.01409807]\n",
      "-6.673391908407211e-05\n",
      "-1.7230623813426468e-05\n",
      "[0.5376452  0.43099064 0.03136405]\n",
      "-0.07529053092002869\n",
      "-0.01943625925147085\n",
      "[0.65830773 0.33038533 0.01130694]\n",
      "-0.3166154632344842\n",
      "-0.08147795490834851\n",
      "[0.43466675 0.5444544  0.02087876]\n",
      "0.13066640309989452\n",
      "0.03371873553615382\n",
      "[0.20625335 0.7105176  0.08322893]\n",
      "0.5874931663274765\n",
      "0.14997444914494992\n",
      "[0.29959717 0.6660782  0.03432473]\n",
      "0.4008057676255703\n",
      "0.10293785622461091\n",
      "[0.6721001  0.3034094  0.02449045]\n",
      "-0.3442002832889557\n",
      "-0.088523229022448\n",
      "[0.2638902  0.670636   0.06547367]\n",
      "0.47221945971250534\n",
      "0.12103023639558809\n",
      "[0.60662955 0.36562985 0.02774058]\n",
      "-0.2132591176778078\n",
      "-0.05497998184793876\n",
      "[0.15474685 0.77854484 0.0667083 ]\n",
      "0.6905063018202782\n",
      "0.17552018821840312\n",
      "[0.39891475 0.56848747 0.03259778]\n",
      "0.2021704912185669\n",
      "0.052129222258672656\n",
      "[0.00406314 0.5305172  0.46541956]\n",
      "0.9918736452236772\n",
      "0.24809393728550902\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mr/lclw3hlj0zs374kh0pvrp2zm0000gn/T/ipykernel_3029/4082368655.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbitcoin_news\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvader_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarity_scores_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbitcoin_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"healine\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/mr/lclw3hlj0zs374kh0pvrp2zm0000gn/T/ipykernel_3029/3917492934.py\u001b[0m in \u001b[0;36mpolarity_scores_roberta\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolarity_scores_roberta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mencoded_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1209\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         )\n\u001b[0;32m--> 846\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    847\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    518\u001b[0m                 )\n\u001b[1;32m    519\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    521\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    406\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 332\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, len(bitcoin_news)):\n",
    "    vader_neg.append(polarity_scores_roberta(bitcoin_news[\"healine\"][i]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6d1ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year_month</th>\n",
       "      <th>year_month_day</th>\n",
       "      <th>healine</th>\n",
       "      <th>link</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202201</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Bitcoin starts 2022 at $47.2K as fresh researc...</td>\n",
       "      <td>https://cointelegraph.com/news/bitcoin-starts-...</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.237430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202201</td>\n",
       "      <td>20220101</td>\n",
       "      <td>The year for Bitcoin: A 2021 roundup of the fl...</td>\n",
       "      <td>https://cointelegraph.com/news/the-year-for-bi...</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.248489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202201</td>\n",
       "      <td>20220101</td>\n",
       "      <td>El Salvador President Nayib Bukele Predicts Tw...</td>\n",
       "      <td>https://www.nasdaq.com/articles/el-salvador-pr...</td>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.243640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202201</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Mexico Announces It Will Issue Its Own Digital...</td>\n",
       "      <td>https://news.bitcoin.com/mexico-announces-it-w...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.246598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>202201</td>\n",
       "      <td>20220101</td>\n",
       "      <td>Kevin O'Leary Reveals Crypto Strategy, Why He ...</td>\n",
       "      <td>https://news.bitcoin.com/kevin-oleary-crypto-s...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.230354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13439</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>202208</td>\n",
       "      <td>20220831</td>\n",
       "      <td>Bitcoin Price Chart May Signal End to Selloff,...</td>\n",
       "      <td>https://www.inferse.com/197765/bitcoin-price-c...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.233234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13440</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>202208</td>\n",
       "      <td>20220831</td>\n",
       "      <td>Crypto will become an inflation hedge — just n...</td>\n",
       "      <td>https://techbullion.com/crypto-will-become-an-...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.166321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13441</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>202208</td>\n",
       "      <td>20220831</td>\n",
       "      <td>Singapore Considers Stricter Rules for Retail ...</td>\n",
       "      <td>https://www.inferse.com/197692/singapore-consi...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.025340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13442</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>202208</td>\n",
       "      <td>20220831</td>\n",
       "      <td>Pimride (PIM) Is Now Available for Trading on ...</td>\n",
       "      <td>https://www.livebitcoinnews.com/pimride-pim-is...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13443</th>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>202208</td>\n",
       "      <td>20220831</td>\n",
       "      <td>What You Need to Know About Running a Cryptocu...</td>\n",
       "      <td>https://www.imcgrupo.com/what-you-need-to-know...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.196895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13444 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  year_month  year_month_day  \\\n",
       "0      2022      1    1      202201        20220101   \n",
       "1      2022      1    1      202201        20220101   \n",
       "2      2022      1    1      202201        20220101   \n",
       "3      2022      1    1      202201        20220101   \n",
       "4      2022      1    1      202201        20220101   \n",
       "...     ...    ...  ...         ...             ...   \n",
       "13439  2022      8   31      202208        20220831   \n",
       "13440  2022      8   31      202208        20220831   \n",
       "13441  2022      8   31      202208        20220831   \n",
       "13442  2022      8   31      202208        20220831   \n",
       "13443  2022      8   31      202208        20220831   \n",
       "\n",
       "                                                 healine  \\\n",
       "0      Bitcoin starts 2022 at $47.2K as fresh researc...   \n",
       "1      The year for Bitcoin: A 2021 roundup of the fl...   \n",
       "2      El Salvador President Nayib Bukele Predicts Tw...   \n",
       "3      Mexico Announces It Will Issue Its Own Digital...   \n",
       "4      Kevin O'Leary Reveals Crypto Strategy, Why He ...   \n",
       "...                                                  ...   \n",
       "13439  Bitcoin Price Chart May Signal End to Selloff,...   \n",
       "13440  Crypto will become an inflation hedge — just n...   \n",
       "13441  Singapore Considers Stricter Rules for Retail ...   \n",
       "13442  Pimride (PIM) Is Now Available for Trading on ...   \n",
       "13443  What You Need to Know About Running a Cryptocu...   \n",
       "\n",
       "                                                    link  sentiment  \\\n",
       "0      https://cointelegraph.com/news/bitcoin-starts-...     0.3182   \n",
       "1      https://cointelegraph.com/news/the-year-for-bi...     0.1027   \n",
       "2      https://www.nasdaq.com/articles/el-salvador-pr...     0.1280   \n",
       "3      https://news.bitcoin.com/mexico-announces-it-w...     0.0000   \n",
       "4      https://news.bitcoin.com/kevin-oleary-crypto-s...     0.0000   \n",
       "...                                                  ...        ...   \n",
       "13439  https://www.inferse.com/197765/bitcoin-price-c...     0.0000   \n",
       "13440  https://techbullion.com/crypto-will-become-an-...     0.0000   \n",
       "13441  https://www.inferse.com/197692/singapore-consi...     0.0000   \n",
       "13442  https://www.livebitcoinnews.com/pimride-pim-is...     0.0000   \n",
       "13443  https://www.imcgrupo.com/what-you-need-to-know...     0.0000   \n",
       "\n",
       "       sentiment2  \n",
       "0        0.237430  \n",
       "1        0.248489  \n",
       "2        0.243640  \n",
       "3        0.246598  \n",
       "4        0.230354  \n",
       "...           ...  \n",
       "13439    0.233234  \n",
       "13440    0.166321  \n",
       "13441   -0.025340  \n",
       "13442    0.248100  \n",
       "13443    0.196895  \n",
       "\n",
       "[13444 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitcoin_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51237bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
